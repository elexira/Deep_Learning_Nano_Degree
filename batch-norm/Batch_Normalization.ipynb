{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization \n",
    "\n",
    "Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf). The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to _layers within_ the network. \n",
    "> It's called **batch** normalization because during training, we normalize each layer's inputs by using the mean and variance of the values in the current *batch*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization – Lesson\n",
    "\n",
    "1. [What is it?](#theory)\n",
    "2. [What are it's benefits?](#benefits)\n",
    "3. [How do we add it to a network?](#implementation_1)\n",
    "4. [Let's see it work!](#demos)\n",
    "5. [What are you hiding?](#implementation_2)\n",
    "\n",
    "# What is Batch Normalization?<a id='theory'></a>\n",
    "\n",
    "Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf). The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to _layers within_ the network. It's called \"batch\" normalization because during training, we normalize each layer's inputs by using the mean and variance of the values in the current mini-batch.\n",
    "\n",
    "Why might this help? Well, we know that normalizing the inputs to a _network_ helps the network learn. But a network is a series of layers, where the output of one layer becomes the input to another. That means we can think of any layer in a neural network as the _first_ layer of a smaller network.\n",
    "\n",
    "For example, imagine a 3 layer network. Instead of just thinking of it as a single network with inputs, layers, and outputs, think of the output of layer 1 as the input to a two layer network. This two layer network would consist of layers 2 and 3 in our original network. \n",
    "\n",
    "Likewise, the output of layer 2 can be thought of as the input to a single layer network, consisting only of layer 3.\n",
    "\n",
    "When you think of it like that - as a series of neural networks feeding into each other - then it's easy to imagine how normalizing the inputs to each layer would help. It's just like normalizing the inputs to any other neural network, but you're doing it at every layer (sub-network).\n",
    "\n",
    "Beyond the intuitive reasons, there are good mathematical reasons why it helps the network learn better, too. It helps combat what the authors call _internal covariate shift_. This discussion is best handled [in the paper](https://arxiv.org/pdf/1502.03167.pdf) and in [Deep Learning](http://www.deeplearningbook.org) a book you can read online written by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Specifically, check out the batch normalization section of [Chapter 8: Optimization for Training Deep Models](http://www.deeplearningbook.org/contents/optimization.html).\n",
    "\n",
    "\n",
    "![batch_norm](./images/lecture1.png)\n",
    "    \n",
    "# Benefits of Batch Normalization<a id=\"benefits\"></a>\n",
    "\n",
    "Batch normalization optimizes network training. It has been shown to have several benefits:\n",
    "1. **Networks train faster** – Each training _iteration_ will actually be slower because of the extra calculations during the forward pass and the additional hyperparameters to train during back propagation. However, it should converge much more quickly, so training should be faster overall. \n",
    "2. **Allows higher learning rates** – Gradient descent usually requires small learning rates for the network to converge. And as networks get deeper, their gradients get smaller during back propagation so they require even more iterations. Using batch normalization allows us to use much higher learning rates, which further increases the speed at which networks train. \n",
    "3. **Makes weights easier to initialize** – Weight initialization can be difficult, and it's even more difficult when creating deeper networks. Batch normalization seems to allow us to be much less careful about choosing our initial starting weights.  \n",
    "4. **Makes more activation functions viable** – Some activation functions do not work well in some situations. Sigmoids lose their gradient pretty quickly, which means they can't be used in deep networks. And ReLUs often die out during training, where they stop learning completely, so we need to be careful about the range of values fed into them. Because batch normalization regulates the values going into each activation function, non-linearlities that don't seem to work well in deep networks actually become viable again.  \n",
    "5. **Simplifies the creation of deeper networks** – Because of the first 4 items listed above, it is easier to build and faster to train deeper neural networks when using batch normalization. And it's been shown that deeper networks generally produce better results, so that's great.\n",
    "6. **Provides a bit of regularlization** – Batch normalization adds a little noise to your network. In some cases, such as in Inception modules, batch normalization has been shown to work as well as dropout. But in general, consider batch normalization as a bit of extra regularization, possibly allowing you to reduce some of the dropout you might add to a network. \n",
    "7. **May give better results overall** – Some tests seem to show batch normalization actually improves the training results. However, it's really an optimization to help train faster, so you shouldn't think of it as a way to make your network better. But since it lets you train networks faster, that means you can iterate over more designs more quickly. It also lets you build deeper networks, which are usually better. So when you factor in everything, you're probably going to end up with better results if you build your networks with batch normalization.\n",
    "    \n",
    "    \n",
    "![](./images/lecture3.png)\n",
    "![](./images/lecture4.png)\n",
    "![](./images/lecture5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the snippet below shows that in case of fully connected layers, the mean and variance is calculated for each feature. number of features in each layer is equal to number of nuerons. For example if you have a FC layer MxN dimension. The output is N dimension, and there will be N means and variances. You need a batch of data to calculate the mean and variance across each feature N. This is because each training data givens you a single value for each N features in a FC layer. \n",
    "![](./images/lecture6.png)\n",
    "![](./images/lecture7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Batch Norm Helps\n",
    "covariate shift: if you train your models on black cats, but in test you see colored cats. Your covariates shift. \n",
    "Covriate shift can happen and propagate across the entire network layers. The Batch normalization ensures that no matter how much incoming values from a previous layer (including the very first input that goes into layer one in the cat example) prior to activation change, what enters the layer has mean and variance beta and gamma. \n",
    "\n",
    "in that sense each layer training becomes a bit more independent of what happened in previous layers, and this speeds up the training and accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization in Images/Tensors\n",
    "\n",
    "based on the code below taken from [mx-net](https://gluon.mxnet.io/chapter04_convolutional-neural-networks/cnn-batch-norm-scratch.html) , the batch normalization in images and tensors is applied to each layer of the tensor. So it is not applied to to each pixel, rather the entire layer. So the very first RGB input image will have 3 means, 3 variance and 3 beta and 3 gamma if you add batch norm to it. The mean and variances are calculated across batches in training, and then the average mean and variances acorss batches is used during test and prediction to normalize the data. Beta and Gamma are estimated via backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def pure_batch_norm(X, gamma, beta, eps = 1e-5):\n",
    "    if len(X.shape) not in (2, 4):\n",
    "        raise ValueError('only supports dense or 2dconv')\n",
    "\n",
    "    # dense\n",
    "    if len(X.shape) == 2:\n",
    "        # mini-batch mean\n",
    "        mean = np.mean(X, axis=0)\n",
    "        print(\"mean shape:\" , mean.shape)\n",
    "        print(\"mean :\", mean)\n",
    "        # mini-batch variance\n",
    "        variance = np.mean((X - mean) ** 2, axis=0)\n",
    "        # normalize\n",
    "        X_hat = (X - mean) * 1.0 / np.sqrt(variance + eps)\n",
    "        # scale and shift\n",
    "        out = gamma * X_hat + beta\n",
    "\n",
    "    # 2d conv\n",
    "    elif len(X.shape) == 4:\n",
    "        # extract the dimensions\n",
    "        N, C, H, W = X.shape\n",
    "        # mini-batch mean\n",
    "        mean = np.mean(X, axis=(0, 2, 3))\n",
    "        print(\"mean shape:\" , mean.shape)\n",
    "        print(\"mean :\", mean)\n",
    "        # mini-batch variance\n",
    "        variance = np.mean((X - mean.reshape((1, C, 1, 1))) ** 2, axis=(0, 2, 3))\n",
    "        # normalize\n",
    "        X_hat = (X - mean.reshape((1, C, 1, 1))) * 1.0 / np.sqrt(variance.reshape((1, C, 1, 1)) + eps)\n",
    "        # scale and shift\n",
    "        out = gamma.reshape((1, C, 1, 1)) * X_hat + beta.reshape((1, C, 1, 1))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input (10, 5)\n",
      "mean shape: (5,)\n",
      "mean : [22.5 23.5 24.5 25.5 26.5]\n"
     ]
    }
   ],
   "source": [
    "#  N = 2 = batch size, D = 5 =  # of neurons in the layer, Assume shape of fully connected layer FC = K*D results\n",
    "# in D output/features that will be normalized and then fed into activation of that layer \n",
    "# and then into the next layer\n",
    "B = np.array(range(10*5)).reshape((10,5)) \n",
    "print(\"shape of input\" , B.shape)\n",
    "output = pure_batch_norm(B, np.array([1,1, 1, 1, 1]), np.array([0,0, 0, 0 , 0]), eps = 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors and Images Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input (2, 3, 2, 4)\n",
      "mean shape: (3,)\n",
      "mean : [15.5 23.5 31.5]\n"
     ]
    }
   ],
   "source": [
    "#  N = 2 = batch size, C = 3 = channels, H = 2 = image height, W = 4 = image width \n",
    "B = np.array(range(2*3*2*4)).reshape((2,3,2,4)) \n",
    "print(\"shape of input\" , B.shape)\n",
    "output = pure_batch_norm(B, np.array([1,1, 1]), np.array([0,0, 0]), eps = 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization in PyTorch<a id=\"implementation_1\"></a>\n",
    "\n",
    "This section of the notebook shows you one way to add batch normalization to a neural network built in PyTorch. \n",
    "\n",
    "The following cells import the packages we need in the notebook and load the MNIST dataset to use in our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 64\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# get the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                            download=True, transform=transform)\n",
    "\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                           download=True, transform=transform)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "                                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff6d44b1f98>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (3,3)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network classes for testing\n",
    "\n",
    "The following class, `NeuralNet`, allows us to create identical neural networks **with and without batch normalization** to compare. The code is heavily documented, but there is also some additional discussion later. You do not need to read through it all before going through the rest of the notebook, but the comments within the code blocks may answer some of your questions.\n",
    "\n",
    "*About the code:*\n",
    ">We are defining a simple MLP for classification; this design choice was made to support the discussion related to batch normalization and not to get the best classification accuracy.\n",
    "\n",
    "### (Important) Model Details\n",
    "\n",
    "There are quite a few comments in the code, so those should answer most of your questions. However, let's take a look at the most important lines.\n",
    "\n",
    "We add batch normalization to layers inside the `__init__` function. Here are some important points about that code:\n",
    "1. Layers with batch normalization do **not** include a bias term.\n",
    "2. We use PyTorch's [BatchNorm1d](https://pytorch.org/docs/stable/nn.html#batchnorm1d) function to handle the math. This is the function you use to operate on linear layer outputs; you'll use [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d) for 2D outputs like filtered images from convolutional layers. \n",
    "3. We add the batch normalization layer **before** calling the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, use_batch_norm, input_size=784, hidden_dim=256, output_size=10):\n",
    "        \"\"\"\n",
    "        Creates a PyTorch net using the given parameters.\n",
    "        \n",
    "        :param use_batch_norm: bool\n",
    "            Pass True to create a network that uses batch normalization; False otherwise\n",
    "            Note: this network will not use batch normalization on layers that do not have an\n",
    "            activation function.\n",
    "        \"\"\"\n",
    "        super(NeuralNet, self).__init__() # init super\n",
    "        \n",
    "        # Default layer sizes\n",
    "        self.input_size = input_size # (28*28 images)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size # (number of classes)\n",
    "        # Keep track of whether or not this network uses batch normalization.\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        # define hidden linear layers, with optional batch norm on their outputs\n",
    "        # layers with batch_norm applied have no bias term\n",
    "        if use_batch_norm:\n",
    "            self.fc1 = nn.Linear(input_size, hidden_dim*2, bias=False)\n",
    "            # input to batch norm = num_features = C, which is from an expected input of size (N,C)\n",
    "            self.batch_norm1 = nn.BatchNorm1d(hidden_dim*2)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(input_size, hidden_dim*2)\n",
    "            \n",
    "        # define *second* hidden linear layers, with optional batch norm on their outputs\n",
    "        if use_batch_norm:\n",
    "            self.fc2 = nn.Linear(hidden_dim*2, hidden_dim, bias=False)\n",
    "            self.batch_norm2 = nn.BatchNorm1d(hidden_dim)\n",
    "        else:\n",
    "            self.fc2 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        \n",
    "        # third and final, fully-connected layer\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # flatten image\n",
    "        x = x.view(-1, 28*28)\n",
    "        # all hidden layers + optional batch norm + relu activation\n",
    "        x = self.fc1(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        # second layer\n",
    "        x = self.fc2(x)\n",
    "        if self.use_batch_norm:\n",
    "            x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        # third layer, no batch norm or activation\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create two different models for testing\n",
    "\n",
    "* `net_batchnorm` is a linear classification model **with** batch normalization applied to the output of its hidden layers\n",
    "* `net_no_norm` is a plain MLP, without batch normalization\n",
    "\n",
    "Besides the normalization layers, everthing about these models is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=False)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=False)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "NeuralNet(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net_batchnorm = NeuralNet(use_batch_norm=True)\n",
    "net_no_norm = NeuralNet(use_batch_norm=False)\n",
    "\n",
    "print(net_batchnorm)\n",
    "print()\n",
    "print(net_no_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "The below `train` function will take in a model and some number of epochs. We'll use cross entropy loss and stochastic gradient descent for optimization. This function returns the losses, recorded after each epoch, so that we can display and compare the behavior of different models.\n",
    "\n",
    "#### `.train()` mode\n",
    "Note that we tell our model whether or not it should be in training mode, `model.train()`. This is an important step because batch normalization has different behavior during training on a batch or testing/evaluating on a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs=10):\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = n_epochs\n",
    "    # track losses\n",
    "    losses = []\n",
    "        \n",
    "    # optimization strategy \n",
    "    # specify loss function (categorical cross-entropy)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    # set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # monitor training loss\n",
    "        train_loss = 0.0\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        batch_count = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update average training loss\n",
    "            train_loss += loss.item() # add up avg batch loss\n",
    "            batch_count +=1                \n",
    "\n",
    "        # print training statistics \n",
    "        losses.append(train_loss/batch_count)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss/batch_count))\n",
    "    \n",
    "    # return all recorded batch losses\n",
    "    return losses\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models\n",
    "\n",
    "In the below cells, we train our two different models and compare their trainining loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.546912\n",
      "Epoch: 2 \tTraining Loss: 0.197372\n",
      "Epoch: 3 \tTraining Loss: 0.135156\n",
      "Epoch: 4 \tTraining Loss: 0.100836\n",
      "Epoch: 5 \tTraining Loss: 0.078061\n",
      "Epoch: 6 \tTraining Loss: 0.061577\n",
      "Epoch: 7 \tTraining Loss: 0.049126\n",
      "Epoch: 8 \tTraining Loss: 0.039432\n",
      "Epoch: 9 \tTraining Loss: 0.031782\n",
      "Epoch: 10 \tTraining Loss: 0.025753\n"
     ]
    }
   ],
   "source": [
    "# batchnorm model losses\n",
    "# this may take some time to train\n",
    "losses_batchnorm = train(net_batchnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.628065\n",
      "Epoch: 2 \tTraining Loss: 0.524036\n",
      "Epoch: 3 \tTraining Loss: 0.379346\n",
      "Epoch: 4 \tTraining Loss: 0.331805\n",
      "Epoch: 5 \tTraining Loss: 0.303105\n",
      "Epoch: 6 \tTraining Loss: 0.280980\n",
      "Epoch: 7 \tTraining Loss: 0.261966\n",
      "Epoch: 8 \tTraining Loss: 0.245059\n",
      "Epoch: 9 \tTraining Loss: 0.229818\n",
      "Epoch: 10 \tTraining Loss: 0.215998\n"
     ]
    }
   ],
   "source": [
    "# *no* norm model losses\n",
    "# you should already start to see a difference in training losses\n",
    "losses_no_norm = train(net_no_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12884f860>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compare\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "#losses_batchnorm = np.array(losses_batchnorm)\n",
    "#losses_no_norm = np.array(losses_no_norm)\n",
    "plt.plot(losses_batchnorm, label='Using batchnorm', alpha=0.5)\n",
    "plt.plot(losses_no_norm, label='No norm', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## Testing\n",
    "\n",
    "You should see that the model with batch normalization, starts off with a lower training loss and, over ten epochs of training, gets to a training loss that is noticeably lower than our model without normalization.\n",
    "\n",
    "Next, let's see how both these models perform on our test data! Below, we have a function `test` that takes in a model and a parameter `train` (True or False) which indicates whether the model should be in training or evaulation mode. This is for comparison purposes, later. This function will calculate some test statistics including the overall test accuracy of a passed in model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, train):\n",
    "    # initialize vars to monitor test loss and accuracy\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    test_loss = 0.0\n",
    "\n",
    "    # set model to train or evaluation mode\n",
    "    # just to see the difference in behavior\n",
    "    if(train==True):\n",
    "        model.train()\n",
    "    if(train==False):\n",
    "        model.eval()\n",
    "    \n",
    "    # loss criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        batch_size = data.size(0)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss += loss.item()*batch_size\n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        for i in range(batch_size):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss/len(test_loader.dataset)))\n",
    "\n",
    "    for i in range(10):\n",
    "        if class_total[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "                str(i), 100 * class_correct[i] / class_total[i],\n",
    "                np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "        100. * np.sum(class_correct) / np.sum(class_total),\n",
    "        np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation Mode\n",
    "\n",
    "Setting a model to evaluation mode is important for models with batch normalization layers!\n",
    "\n",
    ">* Training mode means that the batch normalization layers will use **batch** statistics to calculate the batch norm. \n",
    "* Evaluation mode, on the other hand, uses the estimated **population** mean and variance from the entire training set, which should give us increased performance on this test data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.086881\n",
      "\n",
      "Test Accuracy of     0: 98% (967/980)\n",
      "Test Accuracy of     1: 99% (1126/1135)\n",
      "Test Accuracy of     2: 96% (999/1032)\n",
      "Test Accuracy of     3: 97% (989/1010)\n",
      "Test Accuracy of     4: 96% (952/982)\n",
      "Test Accuracy of     5: 96% (864/892)\n",
      "Test Accuracy of     6: 97% (933/958)\n",
      "Test Accuracy of     7: 96% (990/1028)\n",
      "Test Accuracy of     8: 96% (939/974)\n",
      "Test Accuracy of     9: 95% (966/1009)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9725/10000)\n"
     ]
    }
   ],
   "source": [
    "# test batchnorm case, in *train* mode\n",
    "test(net_batchnorm, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.073484\n",
      "\n",
      "Test Accuracy of     0: 98% (968/980)\n",
      "Test Accuracy of     1: 99% (1127/1135)\n",
      "Test Accuracy of     2: 97% (1005/1032)\n",
      "Test Accuracy of     3: 98% (991/1010)\n",
      "Test Accuracy of     4: 97% (955/982)\n",
      "Test Accuracy of     5: 97% (874/892)\n",
      "Test Accuracy of     6: 97% (932/958)\n",
      "Test Accuracy of     7: 96% (995/1028)\n",
      "Test Accuracy of     8: 96% (940/974)\n",
      "Test Accuracy of     9: 97% (983/1009)\n",
      "\n",
      "Test Accuracy (Overall): 97% (9770/10000)\n"
     ]
    }
   ],
   "source": [
    "# test batchnorm case, in *evaluation* mode\n",
    "test(net_batchnorm, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.207286\n",
      "\n",
      "Test Accuracy of     0: 98% (963/980)\n",
      "Test Accuracy of     1: 98% (1113/1135)\n",
      "Test Accuracy of     2: 91% (943/1032)\n",
      "Test Accuracy of     3: 93% (943/1010)\n",
      "Test Accuracy of     4: 93% (918/982)\n",
      "Test Accuracy of     5: 92% (824/892)\n",
      "Test Accuracy of     6: 95% (912/958)\n",
      "Test Accuracy of     7: 92% (954/1028)\n",
      "Test Accuracy of     8: 91% (891/974)\n",
      "Test Accuracy of     9: 93% (940/1009)\n",
      "\n",
      "Test Accuracy (Overall): 94% (9401/10000)\n"
     ]
    }
   ],
   "source": [
    "# for posterity, test no norm case in eval mode\n",
    "test(net_no_norm, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which model has the highest accuracy?\n",
    "\n",
    "You should see a small improvement whe comparing the batch norm model's accuracy in training and evaluation mode; **evaluation mode** should give a small improvement!\n",
    "\n",
    "You should also see that the model that uses batch norm layers shows a marked improvement in overall accuracy when compared with the no-normalization model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Considerations for other network types\n",
    "\n",
    "This notebook demonstrates batch normalization in a standard neural network with fully connected layers. You can also use batch normalization in other types of networks, but there are some special considerations.\n",
    "\n",
    "### ConvNets\n",
    "\n",
    "Convolution layers consist of multiple feature maps. (Remember, the depth of a convolutional layer refers to its number of feature maps.) And the weights for each feature map are shared across all the inputs that feed into the layer. Because of these differences, batch normalizing convolutional layers requires batch/population mean and variance per feature map rather than per node in the layer.\n",
    "\n",
    "> To apply batch normalization on the outputs of convolutional layers, we use [BatchNorm2d](https://pytorch.org/docs/stable/nn.html#batchnorm2d)\n",
    "\n",
    "\n",
    "### RNNs\n",
    "\n",
    "Batch normalization can work with recurrent neural networks, too, as shown in the 2016 paper [Recurrent Batch Normalization](https://arxiv.org/abs/1603.09025). It's a bit more work to implement, but basically involves calculating the means and variances per time step instead of per layer. You can find an example where someone implemented recurrent batch normalization in PyTorch, in [this GitHub repo](https://github.com/jihunchoi/recurrent-batch-normalization-pytorch)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision_class",
   "language": "python",
   "name": "comp_vis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
