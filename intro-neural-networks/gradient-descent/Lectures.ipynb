{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# watch the perceptron video "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the discussion is about this linear function that they lecture notes associates with the perceptron\n",
    "Here is a one interpretation, it might be wrong\n",
    "consider logistic regression case where we want to model it with one perceptron, ignore bias\n",
    "assume 2 classes:\n",
    "\n",
    "P(Y = class2| x1, x2) = sigmoid(w1x1 + w2x2)\n",
    "the decision boundry is where the decision can change\n",
    "P(Y = class2| x1, x2) = 0.5 \n",
    "sigmoid(w1x1 + w2x2) = 0.5\n",
    "or you can say anything that satisfice the following is in class 1\n",
    "sigmoid(w1x1 + w2x2) > 0.5\n",
    "when you solve this you get the following equation \n",
    "w1x1 + w2x2 = 0 or x2 = -(w1/w2)x1 \n",
    "you can plot this in a 2D plane, with two axis, x1 and x2\n",
    "the line had a negative slope \n",
    "\n",
    "in case of \n",
    "sigmoid(w1x1 + w2x2) > 0.5\n",
    "the equation becomes\n",
    "w1x1 + w2x2 > 0  or x2 > -(w1/w2)x1 \n",
    "\n",
    "assume w1 = w2 = 1\n",
    "\n",
    "equation becomes => x2 = -x1   or x2 > -x1  or x1 + x2 > 0 which is points to the right side of plane x1 + x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron with x1, x2 heat map \n",
    "watch also the class video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "x1 = np.linspace(0, 1, 10)\n",
    "x2 = np.linspace(0, 1, 10)\n",
    "pts = itertools.product(x1, x2)\n",
    "# the lower the weight the more it becomes OR , where both X1, X2 have to be large to activate sigmoid\n",
    "# higher weight makes it more like AND, where as long as one of X1 and X2 is active the segmoid is active\n",
    "# with negative weights you get X1 has to be high and X1 low for activation\n",
    "w1 = .00010  \n",
    "w2 = .00010\n",
    "\n",
    "z = list()\n",
    "x = list()\n",
    "y = list()\n",
    "for i, p in enumerate(pts):\n",
    "    x.append(p[0])\n",
    "    y.append(p[1])\n",
    "    z.append(np.exp(w1*p[0] + w2*p[1])/(1 + np.exp(w1*p[0] + w2*p[1])))\n",
    "\n",
    "\n",
    "x=np.array(x)\n",
    "y=np.array(y)\n",
    "z=np.array(z)\n",
    "\n",
    "\n",
    "x=np.unique(x)\n",
    "y=np.unique(y)\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "Z=z.reshape(len(y),len(x))\n",
    "\n",
    "plt.pcolormesh(X,Y,Z)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    # more general : a sample belongs to only one class K, you want to maximize that \n",
    "    # multiply and indicator k, 1{if Y = k} with its corresponding prediction probability from model \n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/lecture1.png)\n",
    "![](./images/lecture2.png)\n",
    "![](./images/lecture3.png)\n",
    "![](./images/lecture4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient update for a simple feed-forward network\n",
    "1 neuron in this case with a 2D input\n",
    "\n",
    "$$\n",
    "y = w_1x_1 + w_2x_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} =  \\sigma (w_1x_1 + w_2x_2)\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "e = y - \\hat{y} =  y - \\sigma (w_1x_1 + w_2x_2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{de^2}{dw_1} = \\frac{d(y - \\sigma (w_1x_1 + w_2x_2))^2}{dw_1} = \\frac{de^2}{d\\hat{y}}\\frac{d\\hat{y}}{d\\sigma}\\frac{d\\sigma}{dw_1} = -2 (e) \\sigma (1- \\sigma) x_1 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Defining the sigmoid function for activations\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Derivative of the sigmoid function\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Input data\n",
    "x = np.array([0.1, 0.3])\n",
    "# Target\n",
    "y = 0.2\n",
    "# Input to output weights\n",
    "weights = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate, eta in the weight step equation\n",
    "learnrate = 0.5\n",
    "\n",
    "# the linear combination performed by the node (h in f(h) and f'(h))\n",
    "h = x[0]*weights[0] + x[1]*weights[1]\n",
    "# or h = np.dot(x, weights)\n",
    "\n",
    "# The neural network output (y-hat)\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# output error (y - y-hat)\n",
    "error = y - nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# error term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# Gradient descent step \n",
    "del_w = [ learnrate * error_term * x[0],\n",
    "          learnrate * error_term * x[1]]\n",
    "# or del_w = learnrate * error_term * x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.6899744811276125\n",
      "Amount of Error:\n",
      "-0.1899744811276125\n",
      "Change in Weights:\n",
      "[-0.02031869 -0.04063738 -0.06095608 -0.08127477]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consolidated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# TODO: Calculate the node's linear combination of inputs and weights\n",
    "h = np.dot(x, w)\n",
    "\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "error_term = error * sigmoid_prime(h)\n",
    "# Note: The sigmoid_prime function calculates sigmoid(h) twice,\n",
    "#       but you've already calculated it once. You can make this\n",
    "#       code more efficient by calculating the derivative directly\n",
    "#       rather than calling sigmoid_prime, like this:\n",
    "# error_term = error * nn_output * (1 - nn_output)\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Weight Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.iloc[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.26276093849966364\n",
      "Train loss:  0.20928619409324895\n",
      "Train loss:  0.20084292908073417\n",
      "Train loss:  0.1986215647552789\n",
      "Train loss:  0.19779851396686018\n",
      "Train loss:  0.19742577912189863\n",
      "Train loss:  0.19723507746241065\n",
      "Train loss:  0.19712945625092465\n",
      "Train loss:  0.19706766341315077\n",
      "Train loss:  0.19703005801777368\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Activation of the output unit\n",
    "        #   Notice we multiply the inputs and the weights here \n",
    "        #   rather than storing h as a separate variable \n",
    "        output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "        # The error, the target minus the network output\n",
    "        error = y - output\n",
    "\n",
    "        # The error term\n",
    "        #   Notice we calulate f'(h) here instead of defining a separate\n",
    "        #   sigmoid_prime function. This just makes it faster because we\n",
    "        #   can re-use the result of the sigmoid function stored in\n",
    "        #   the output variable\n",
    "        error_term = error * output * (1 - output)\n",
    "\n",
    "        # The gradient descent step, the error times the gradient times the inputs\n",
    "        del_w += error_term * x\n",
    "\n",
    "    # Update the weights here. The learning rate times the \n",
    "    # change in weights, divided by the number of records to average\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Layer Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[0.00804047 0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ 1.77005547e-04 -5.11178506e-04]\n",
      " [ 3.54011093e-05 -1.02235701e-04]\n",
      " [-7.08022187e-05  2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "output_error_term = error * output * (1 - output)\n",
    "\n",
    "# TODO: Calculate error term for hidden layer\n",
    "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \\\n",
    "                    hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementing BackPropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.27615555613131737\n",
      "Train loss:  0.25188749531892873\n",
      "Train loss:  0.23865913377107018\n",
      "Train loss:  0.2314109939057871\n",
      "Train loss:  0.2273142944439588\n",
      "Train loss:  0.22490954122608203\n",
      "Train loss:  0.2234450815658325\n",
      "Train loss:  0.2225237489529173\n",
      "Train loss:  0.2219284620157016\n",
      "Train loss:  0.2215363596614336\n",
      "Prediction accuracy: 0.750\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 2000\n",
    "learnrate = 0.05\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "        output = sigmoid(np.dot(hidden_output,\n",
    "                                weights_hidden_output))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_error_term = error * output * (1 - output)\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
    "\n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "\n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += output_error_term * hidden_output\n",
    "        del_w_input_hidden += hidden_error_term * x[:, None]\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
