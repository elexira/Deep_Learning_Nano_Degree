{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the discriminator (D) output as a probability density function, you give it an image, if it is real, the probability is close to 1 and if not close to 0. The shape of pdf(x) is very nonlinear of course. The job of the generator is to map input noise to an output x - which can be an image, where pdf(x) is high. Think of x as a random variable with a pdf like normal-pdf or weibull-pdf. In parametrical statistics, we would first a proabilistic assumption about x's like they come from a normal pdf. Then we will take mean and varianve of sample of x's and that allow us to estimate the pdf of x. here we are trying to estimate any general pdf and then generate samples from this distribution.\n",
    "\n",
    "\n",
    "This process is similar to importance/gibbs/MHasting sampling where we draw samples and then reject or accept them depending on a criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equilibirum is a point in the game where neither players can do any better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Training Logic:\n",
    "I think it is faily easy to understand how the discriminator loss works. The generator loss is tricker, because the course instructions keeps saying it is trained on the flipped labels. \n",
    "\n",
    "this is how i explain it.\n",
    "first during training of generator we only update the parameters of the generator. We do not mess with discriminator parameters, as shown below:\n",
    "```\n",
    "        # Update the parameters in the optimizer.\n",
    "        g_optimizer.step()\n",
    "```\n",
    "so we start with some random inputs, and then pass it to the discriminator for final score. \n",
    "so what happens to z2 ? it gets bigger and bigger inside Generator, but then when it gets to Discriminator, it shrinks down to eventually one score [0, 1] as shown below (look at the trpazoids)\n",
    "\n",
    "<img src='assets/gan_pipeline.png' width=70% />\n",
    "\n",
    "here is the code that sends z2 to generator and then to the discriminator. \n",
    "\n",
    "```\n",
    "        # Generate fake images\n",
    "        z2 = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "        z2 = torch.from_numpy(z2).float()\n",
    "        z2 = z2.cuda()\n",
    "        generator_training_fake_images = G(z2)\n",
    "        # Compute the discriminator losses on fake images \n",
    "        # using flipped labels!\n",
    "        discriminator_score_fake_images = D(generator_training_fake_images)\n",
    "```\n",
    "We would like to train the Generator so the final score from discriminator is 1. That is, if the score is far from 1 currently and closer to 0, we want to train Generator parameters during generator training (not the Discriminator parameters), so that the discriminator final score becomes close to 1. So we adjust the initial layers that belong to Generator to accomplish this since we cannot touch the Discriminator at this point. This is  exactly the opposite of transfer learning, for example in Resnet, in which we adjust the final layers of a deep network, as opposed to initial layers here, to achieve higher classification performance. In transger learning we do not touch the initial layers, and can only train the final layers. \n",
    "\n",
    "notice if you include the discriminator in the training of generator with labels of 1, you would essentially train the discriminator (along with generator) to say any garbage that generator produces is real. So it is critical you do not include discriminator parameters during generator training. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision_class",
   "language": "python",
   "name": "comp_vis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
